{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use `tf.estimator` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    num_epochs=None,\n",
    "    shuffle=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_val},\n",
    "    y=y_val,\n",
    "    num_epochs=1,\n",
    "    shuffle=False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=y_train,\n",
    "    batch_size=256,\n",
    "    num_epochs=1,\n",
    "    shuffle=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN of my own design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cnn(features, labels, mode):\n",
    "    \n",
    "    X = tf.cast(features[\"x\"], tf.float32)    \n",
    "    reg = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "    # convolutional layers\n",
    "    h1 = tf.layers.conv2d(X, 32, 3, activation=tf.nn.relu, kernel_regularizer=reg)\n",
    "    h1_pool = tf.layers.max_pooling2d(h1, (2, 2), (2, 2))\n",
    "    h1_batchnorm = tf.layers.batch_normalization(h1_pool)\n",
    "    \n",
    "    h2 = tf.layers.conv2d(h1_batchnorm, 64, 2, activation=tf.nn.relu, kernel_regularizer=reg)\n",
    "    h2_pool = tf.layers.max_pooling2d(h2, (2, 2), (2, 2))\n",
    "    h2_batchnorm = tf.layers.batch_normalization(h2_pool)\n",
    "    \n",
    "    h3 = tf.layers.conv2d(h2_batchnorm, 128, 2, activation=tf.nn.relu, kernel_regularizer=reg)\n",
    "    h3_batchnorm = tf.layers.batch_normalization(h3)\n",
    "    # dense layers\n",
    "    d0 = tf.layers.flatten(h3_batchnorm)\n",
    "    \n",
    "    d1 = tf.layers.dense(d0, 1024, activation=tf.nn.relu, kernel_regularizer=reg)\n",
    "    d1_batchnorm = tf.layers.batch_normalization(d1)    \n",
    "    d2 = tf.layers.dense(d1_batchnorm, 10, kernel_regularizer=reg)\n",
    "    \n",
    "    y_out = d2\n",
    "        \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=y_out, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(y_out, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    onehot_labels = tf.one_hot(indices=labels, depth=10)\n",
    "    softmax_loss = tf.losses.softmax_cross_entropy(logits=y_out, onehot_labels=onehot_labels)\n",
    "    loss = l2_loss + softmax_loss\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    eval_metric = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cnn_classifier = tf.estimator.Estimator(model_fn=my_cnn, model_dir='/tmp/my_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cnn_classifier.train(input_fn=train_input_fn, steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = my_cnn_classifier.evaluate(input_fn=test_input_fn)\n",
    "val_results = my_cnn_classifier.evaluate(input_fn=val_input_fn)\n",
    "train_results = my_cnn_classifier.evaluate(input_fn=train_test_input_fn)\n",
    "print('val', val_results)\n",
    "print('test', test_results)\n",
    "print('train', train_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measured accuracy:\n",
    "* Validation set: 73.0%\n",
    "* Test set: 71.3%\n",
    "* Training set: 85.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
